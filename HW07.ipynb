{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5a594c",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646407a",
   "metadata": {},
   "source": [
    "## (a) \n",
    "### Hidden layer:\n",
    "\n",
    "1D Input → use shallow network (1–2 hidden layers)\n",
    "\n",
    "2D or more dimension input → may need deeper network\n",
    "\n",
    "### Node number:\n",
    "\n",
    "Smooth, simple function→ small width (16–32)\n",
    "\n",
    "Oscillatory / curved function→ moderate width (32–64)\n",
    "\n",
    "Very complex function → larger width or extra layer\n",
    "\n",
    "For this question, I choose 1 hidden layer and 32 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a0fce",
   "metadata": {},
   "source": [
    "## (b)\n",
    "### Hidden layers: \n",
    "\n",
    "use ReLU as a good first choice (remember, it's just a good default, not a mandatory one).\n",
    "\n",
    "### Output layer:\n",
    "\n",
    "Regression (predict a real number, e.g., stock price differences): use Linear output.\n",
    "\n",
    "Regression (predict a non-negative number, e.g., heights): use ReLU output. A special case of this is - predicting values between 0 and 1, say Probabilities - use Sigmoid output.\n",
    "\n",
    "Binary classification (yes/no): use Sigmoid output (one node).\n",
    "\n",
    "Multi-class classification (one of many classes): use Softmax (not covered yet in class) - don’t use a single sigmoid for this.\n",
    "\n",
    "### My answer:\n",
    "Hidden layer: ReLU\n",
    "\n",
    "output layer: Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296329e7",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a04e23",
   "metadata": {},
   "source": [
    "![Loss Function](loss_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a445579",
   "metadata": {},
   "source": [
    "based on this rule, I choose MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688eb34",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bf216",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcc4743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b868543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hw7_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82f513",
   "metadata": {},
   "source": [
    "## Split data into two columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21dce757",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d562db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9e695",
   "metadata": {},
   "source": [
    "## Nomralize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1455bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized = (x - x.mean()) / x.std()\n",
    "y_normalized = (y - y.mean()) / y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9056d306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0a3c6",
   "metadata": {},
   "source": [
    "## Convert data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42a874f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46aa78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = torch.tensor(x_normalized, dtype=torch.float32).view(-1, 1)\n",
    "y_t = torch.tensor(y_normalized, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Convert the normalized data into a 2-dimensional float tensor that the neural network can read and process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1c61c",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81722969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b244612",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_t, y_t)\n",
    "# Combine input tensor (x_t) and target tensor (y_t) into a dataset\n",
    "# Each sample becomes a pair: (x, y)\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# shuffle the order of pairs of data in the dataset and then splist the dataset into mini-batches\n",
    "# 99 data & 32 batch --> batch1 32 pairs of data, batch2 32 pairs of data\n",
    "# batch3 32 pairs of data, batch4 3 pairs of data\n",
    "\n",
    "\n",
    "# A small dataset generally means fewer than 10,000 samples, \n",
    "# while fewer than 1,000 samples is considered very small. \n",
    "# dataset with 99 samples is considered very small or tiny.\n",
    "# For small datasets, it is recommended to use smaller batch sizes such as 8, 16, or 32.\n",
    "# In general, smaller datasets prefer smaller batches, and larger datasets can handle larger batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c380c3b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2245fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e8a4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(     # generate the NNs\n",
    "    nn.Linear(1, 32),      # the first hidden layer with 32 neurons (you choose the num of neurons before)\n",
    "    nn.ReLU(),             # activation function (activation functions introduce non-linearity into the model)\n",
    "    nn.Linear(32, 1)       # the output layer \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c69ca",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7393b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# There are many optimizers. You may want to learn how to choose the loss_f and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a1253",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bccba4",
   "metadata": {},
   "source": [
    "For small datasets like yours (99 samples)\n",
    "\n",
    "Try 200–1000 epochs\n",
    "\n",
    "Small datasets often need more epochs because each epoch sees very little data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a3fa485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4851933717727661\n",
      "Epoch 50, Loss: 0.7577405571937561\n",
      "Epoch 100, Loss: 0.8178079128265381\n",
      "Epoch 150, Loss: 0.5439125895500183\n",
      "Epoch 200, Loss: 0.8879322409629822\n",
      "Epoch 250, Loss: 0.7554197311401367\n",
      "Epoch 300, Loss: 0.8804777264595032\n",
      "Epoch 350, Loss: 2.309202194213867\n",
      "Epoch 400, Loss: 1.8837543725967407\n",
      "Epoch 450, Loss: 1.344854712486267\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    for x, y in loader:\n",
    "        # Forward pass (model makes prediction)\n",
    "        forw = model(x)\n",
    "\n",
    "        # compute loss (measure prediction error)\n",
    "        loss = loss_fn(forw, y)\n",
    "\n",
    "        # Backward pass (compute gradients to know How much the loss will change if we change this weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights (Adjust weights to reduce loss actomatically)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print (f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "        # We print training information to monitor learning progress, detect problems,\n",
    "        # and decide when to stop training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
